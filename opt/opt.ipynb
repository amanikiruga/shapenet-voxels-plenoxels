{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "883d3a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 Alex Yu\n",
    "\n",
    "# First, install svox2\n",
    "# Then, python opt.py <path_to>/nerf_synthetic/<scene> -t ckpt/<some_name>\n",
    "# or use launching script:   sh launch.sh <EXP_NAME> <GPU> <DATA_DIR>\n",
    "MAIN_DIR = \"/om/user/akiruga/svox2/opt\"\n",
    "import sys \n",
    "sys.path.append(MAIN_DIR)\n",
    "import torch\n",
    "import torch.cuda\n",
    "import torch.optim\n",
    "import torch.nn.functional as F\n",
    "import svox2\n",
    "import json\n",
    "import imageio\n",
    "import os\n",
    "from os import path\n",
    "import shutil\n",
    "import gc\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import math\n",
    "import argparse\n",
    "import cv2\n",
    "from util.dataset import datasets\n",
    "from util.util import Timing, get_expon_lr_func, generate_dirs_equirect, viridis_cmap\n",
    "from util import config_util\n",
    "\n",
    "from warnings import warn\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import NamedTuple, Optional, Union\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de947618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create argument namespace object to mimic argparse\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# Set all the default argument values as attributes\n",
    "args.train_dir = '/om/user/akiruga/svox2/data/ckpts/shapenet_chairs_jupyter' # ckpts\n",
    "# args.reso = \"[[32, 32, 32]]\"\n",
    "# args.reso = \"[[32, 32, 32]]\"  # Using 32x32x32 with grid scaling for focused resolution\n",
    "args.reso = \"[[32, 32, 32]]\"  # Using 32x32x32 with grid scaling for focused resolution\n",
    "args.upsamp_every = 3 * 12800\n",
    "args.init_iters = 0\n",
    "args.upsample_density_add = 0.0\n",
    "args.basis_type = 'sh'\n",
    "args.basis_reso = 32\n",
    "args.sh_dim = 9\n",
    "args.mlp_posenc_size = 4\n",
    "args.mlp_width = 32\n",
    "args.background_nlayers = 0\n",
    "args.background_reso = 512\n",
    "args.n_iters = 10 * 12800\n",
    "args.batch_size = 5000\n",
    "\n",
    "# Grid scaling to focus 32x32x32 resolution on object region\n",
    "# Current: object ~82% of image (105/128), Target: ~60% of image\n",
    "# Scale factor: 60/82 ≈ 0.73 makes grid tighter around object\n",
    "args.grid_scale_factor = 0.45  # Scale down scene radius to focus grid\n",
    "\n",
    "args.sigma_optim = 'rmsprop'\n",
    "args.lr_sigma = 3e1\n",
    "args.lr_sigma_final = 5e-2\n",
    "args.lr_sigma_decay_steps = 250000\n",
    "args.lr_sigma_delay_steps = 15000\n",
    "args.lr_sigma_delay_mult = 1e-2\n",
    "\n",
    "args.sh_optim = 'rmsprop'\n",
    "args.lr_sh = 1e-2\n",
    "args.lr_sh_final = 5e-6\n",
    "args.lr_sh_decay_steps = 250000\n",
    "args.lr_sh_delay_steps = 0\n",
    "args.lr_sh_delay_mult = 1e-2\n",
    "\n",
    "args.lr_fg_begin_step = 0\n",
    "\n",
    "args.bg_optim = 'rmsprop'\n",
    "args.lr_sigma_bg = 3e0\n",
    "args.lr_sigma_bg_final = 3e-3\n",
    "args.lr_sigma_bg_decay_steps = 250000\n",
    "args.lr_sigma_bg_delay_steps = 0\n",
    "args.lr_sigma_bg_delay_mult = 1e-2\n",
    "\n",
    "args.lr_color_bg = 1e-1\n",
    "args.lr_color_bg_final = 5e-6\n",
    "args.lr_color_bg_decay_steps = 250000\n",
    "args.lr_color_bg_delay_steps = 0\n",
    "args.lr_color_bg_delay_mult = 1e-2\n",
    "\n",
    "args.basis_optim = 'rmsprop'\n",
    "args.lr_basis = 1e-6\n",
    "args.lr_basis_final = 1e-6\n",
    "args.lr_basis_decay_steps = 250000\n",
    "args.lr_basis_delay_steps = 0\n",
    "args.lr_basis_begin_step = 0\n",
    "args.lr_basis_delay_mult = 1e-2\n",
    "\n",
    "args.rms_beta = 0.95\n",
    "args.print_every = 20\n",
    "args.save_every = 5\n",
    "args.eval_every = 1\n",
    "\n",
    "args.init_sigma = 0.1\n",
    "args.init_sigma_bg = 0.1\n",
    "\n",
    "args.log_mse_image = True\n",
    "args.log_depth_map = True\n",
    "args.log_depth_map_use_thresh = None\n",
    "\n",
    "args.thresh_type = \"weight\"\n",
    "args.weight_thresh = 0.0005 * 512\n",
    "args.density_thresh = 5.0\n",
    "args.background_density_thresh = 1.0+1e-9\n",
    "args.max_grid_elements = 44_000_000\n",
    "\n",
    "args.tune_mode = False\n",
    "args.tune_nosave = False\n",
    "\n",
    "args.lambda_tv = 1e-5\n",
    "args.tv_sparsity = 0.01\n",
    "args.tv_logalpha = False\n",
    "args.lambda_tv_sh = 1e-3\n",
    "args.tv_sh_sparsity = 0.01\n",
    "args.lambda_tv_lumisphere = 0.0\n",
    "args.tv_lumisphere_sparsity = 0.01\n",
    "args.tv_lumisphere_dir_factor = 0.0\n",
    "args.tv_decay = 1.0\n",
    "args.lambda_l2_sh = 0.0\n",
    "args.tv_early_only = 1\n",
    "args.tv_contiguous = 1\n",
    "\n",
    "args.lambda_sparsity = 0.0\n",
    "args.lambda_beta = 0.0\n",
    "\n",
    "args.lambda_tv_background_sigma = 1e-2\n",
    "args.lambda_tv_background_color = 1e-2\n",
    "args.tv_background_sparsity = 0.01\n",
    "\n",
    "args.lambda_tv_basis = 0.0\n",
    "\n",
    "args.weight_decay_sigma = 1.0\n",
    "args.weight_decay_sh = 1.0\n",
    "\n",
    "args.lr_decay = True\n",
    "args.n_train = None\n",
    "args.nosphereinit = True\n",
    "\n",
    "# Add common args from config_util\n",
    "args.data_dir = \"/om/user/akiruga/datasets/srn_chairs_alternate_views/feab80af7f3e459120523e15ec10a342/viz\"\n",
    "# args.data_dir = \"/weka/scratch/weka/tenenbaum/akiruga/svox2/data/datasets/lego_real_night_radial\"\n",
    "args.config = None\n",
    "args.dataset_type = \"shapenet\" # \"auto\"\n",
    "args.scene_scale = None\n",
    "args.scale = None\n",
    "args.seq_id = 1000\n",
    "args.epoch_size = 12800\n",
    "args.white_bkgd = True\n",
    "args.llffhold = 8\n",
    "args.normalize_by_bbox = False\n",
    "args.data_bbox_scale = 1.2\n",
    "args.cam_scale_factor = 0.95\n",
    "args.normalize_by_camera = False\n",
    "args.perm = False\n",
    "args.step_size = 0.5 # 0.0125\n",
    "args.sigma_thresh = 1e-8\n",
    "args.stop_thresh = 1e-7\n",
    "args.background_brightness = 1.0\n",
    "args.renderer_backend = 'cuvol'\n",
    "args.random_sigma_std = 0.0\n",
    "args.random_sigma_std_background = 0.0\n",
    "args.near_clip = 0.00\n",
    "args.use_spheric_clip = False\n",
    "args.enable_random = False\n",
    "args.last_sample_opaque = False\n",
    "\n",
    "\n",
    "# data specific args \n",
    "args.fov = 51.98948897809546\n",
    "args.znear = 1.25\n",
    "args.zfar = 2.75\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "688a8be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_video(grid,\n",
    "                 cameras,\n",
    "                 out_path,\n",
    "                 fps: int = 12,\n",
    "                 crop: float = 1.0):\n",
    "    \"\"\"\n",
    "    Render a simple orbit video of the current grid and dump to MP4.\n",
    "    Args:\n",
    "        grid (svox2.SparseGrid): trained / training grid\n",
    "        cameras (List[svox2.Camera]): list of camera poses\n",
    "        out_path (str | Path): where to write the mp4\n",
    "        fps (int): frames per second\n",
    "        crop (float): 1.0 = full res, <1 crops center\n",
    "    \"\"\"\n",
    "    grid.eval()                        # just for safety\n",
    "    frames = []\n",
    "    with torch.no_grad():\n",
    "        for cam in cameras:\n",
    "            # Optional center‑crop so you can render faster mid‑training\n",
    "            w, h = cam.width, cam.height\n",
    "            if crop < 1.0:\n",
    "                cam = svox2.Camera(\n",
    "                    cam.c2w, cam.fx, cam.fy,\n",
    "                    cam.cx * crop, cam.cy * crop,\n",
    "                    int(w * crop), int(h * crop),\n",
    "                    ndc_coeffs=cam.ndc_coeffs\n",
    "                )\n",
    "            im = grid.volume_render_image(cam, use_kernel=True)\n",
    "            im = (im.clamp(0, 1).cpu().numpy() * 255).astype(np.uint8)\n",
    "            frames.append(im)\n",
    "    imageio.mimwrite(str(out_path), frames, fps=fps, macro_block_size=8)\n",
    "    print(f\"✔️  Saved preview video → {out_path}\")\n",
    "    grid.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8999687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate grid center and size using camera-aware approach (similar to GaussianVoxelGrid)\n",
    "def calculate_grid_center_and_radius(cfg, camera_centers, cameras):\n",
    "    \"\"\"Calculate optimal grid center and radius based on camera geometry\"\"\"\n",
    "    \n",
    "    # Extract camera data like similarity_from_cameras does\n",
    "    c2w = torch.stack([cam.c2w for cam in cameras]).cpu().numpy()  # Move to CPU first\n",
    "    t = c2w[:, :3, 3]  # Camera positions\n",
    "    R = c2w[:, :3, :3]  # Camera rotations\n",
    "    \n",
    "    # Get forward directions (where cameras are pointing) \n",
    "    fwds = np.sum(R * np.array([0, 0.0, 1.0]), axis=-1)\n",
    "    \n",
    "    # For each camera ray, find the closest point to the mean camera position\n",
    "    # This finds where cameras are actually looking (the object center)\n",
    "    mean_cam_pos = np.mean(t, axis=0)\n",
    "    \n",
    "    closest_points = []\n",
    "    for i in range(len(t)):\n",
    "        # For ray: P(s) = t[i] + s * fwds[i]\n",
    "        # Find closest point on ray to mean_cam_pos\n",
    "        ray_param = np.dot(mean_cam_pos - t[i], fwds[i])\n",
    "        closest_point = t[i] + ray_param * fwds[i]\n",
    "        closest_points.append(closest_point)\n",
    "    \n",
    "    # The object center is the median of these closest points (same as similarity_from_cameras)\n",
    "    object_center = np.median(closest_points, axis=0)\n",
    "    \n",
    "    # Calculate grid dimensions based on FOV and z-range\n",
    "    radius = np.linalg.norm(mean_cam_pos)\n",
    "    fov_rad = np.deg2rad(cfg.fov)\n",
    "    half_xy = radius * np.tan(0.5 * fov_rad) * 0.6  # 0.6 is a safety factor\n",
    "    half_z = 0.3 * (cfg.zfar - cfg.znear)\n",
    "    \n",
    "    # Make it cubic (optional, or keep separate dimensions)\n",
    "    half_extent = max(half_xy, half_z)\n",
    "    \n",
    "    return object_center, [half_extent, half_extent, half_extent]\n",
    "\n",
    "\n",
    "# Add this function to save comparison images during eval\n",
    "def save_comparison_image(grid, dset_test, epoch_id, args, device):\n",
    "    \"\"\"Save a single comparison image (rendered vs GT side by side)\"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Use the first test image\n",
    "        img_id = 0\n",
    "        c2w = dset_test.c2w[img_id].to(device=device)\n",
    "        cam = svox2.Camera(c2w,\n",
    "                           dset_test.intrins.get('fx', img_id),\n",
    "                           dset_test.intrins.get('fy', img_id),\n",
    "                           dset_test.intrins.get('cx', img_id),\n",
    "                           dset_test.intrins.get('cy', img_id),\n",
    "                           width=dset_test.get_image_size(img_id)[1],\n",
    "                           height=dset_test.get_image_size(img_id)[0],\n",
    "                           ndc_coeffs=dset_test.ndc_coeffs)\n",
    "        \n",
    "        # Render the image\n",
    "        rgb_pred = grid.volume_render_image(cam, use_kernel=True)\n",
    "        rgb_gt = dset_test.gt[img_id].to(device=device)\n",
    "        \n",
    "        # Convert to numpy and scale to 0-255\n",
    "        pred_img = (rgb_pred.clamp(0, 1).cpu().numpy() * 255).astype(np.uint8)\n",
    "        gt_img = (rgb_gt.clamp(0, 1).cpu().numpy() * 255).astype(np.uint8)\n",
    "        \n",
    "        # Create side-by-side comparison (rendered on left, GT on right)\n",
    "        comparison = np.concatenate([pred_img, gt_img], axis=1)\n",
    "        \n",
    "        # Save the comparison image\n",
    "        comparison_path = Path(args.train_dir) / f\"comparison_epoch_{epoch_id:04d}.png\"\n",
    "        imageio.imwrite(str(comparison_path), comparison)\n",
    "        print(f\"📸 Saved comparison image: {comparison_path}\")\n",
    "        \n",
    "        return comparison_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1078d4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOAD NSVF DATA /om/user/akiruga/datasets/srn_chairs_alternate_views/feab80af7f3e459120523e15ec10a342/viz split train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                            | 0/187 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 187/187 [00:00<00:00, 689.11it/s]\n",
      "/om/user/akiruga/sw/envs/RCDM/lib/python3.10/site-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMALIZE BY? manual\n",
      "scene_scale 1.0\n",
      " intrinsics (loaded reso) Intrin(fx=131.25, fy=131.25, cx=64.0, cy=64.0)\n",
      " Generating rays, scaling factor 1\n",
      "LOAD NSVF DATA /om/user/akiruga/datasets/srn_chairs_alternate_views/feab80af7f3e459120523e15ec10a342/viz split test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:00<00:00, 634.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMALIZE BY? manual\n",
      "scene_scale 1.0\n",
      " intrinsics (loaded reso) Intrin(fx=131.25, fy=131.25, cx=64.0, cy=64.0)\n",
      "  FOV: 51.98948897809546 degrees = 0.9073877590970678 radians\n",
      "  Image size: 128x128\n",
      "  Calculated focal length: 131.25\n",
      "  Principal point: (64.0, 64.0)\n",
      "Calculated optimal center: [-0.00228033  0.00017712  0.00033823]\n",
      "Calculated optimal radius: [0.44999999999999996, 0.44999999999999996, 0.44999999999999996]\n",
      "Custom center: [0.0, 0.0, 0.0]\n",
      "Original dataset center: [0.0, 0.0, 0.0]\n",
      "Original scene radius: [1.0, 1.0, 1.0]\n",
      "Scaled scene radius: [0.45, 0.45, 0.45] (scale factor: 0.45)\n",
      "Render options RenderOptions(backend='cuvol', background_brightness=1.0, step_size=0.5, sigma_thresh=1e-08, stop_thresh=1e-07, last_sample_opaque=False, near_clip=0.0, use_spheric_clip=False, random_sigma_std=0.0, random_sigma_std_background=0.0)\n",
      "Setting temporary gray density to visualize grid bounds...\n",
      "✔️  Saved preview video → /om/user/akiruga/svox2/data/ckpts/shapenet_chairs_jupyter/video_00000_bounds.mp4\n",
      "✅ Grid bounds video saved! Density reset to 0.1 for training\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(args.train_dir, exist_ok=True)\n",
    "summary_writer = SummaryWriter(args.train_dir)\n",
    "\n",
    "reso_list = json.loads(args.reso)\n",
    "reso_id = 0\n",
    "\n",
    "with open(path.join(args.train_dir, 'args.json'), 'w') as f:\n",
    "    json.dump(args.__dict__, f, indent=2)\n",
    "    # Changed name to prevent errors\n",
    "    # shutil.copyfile(__file__, path.join(args.train_dir, 'opt_frozen.py'))\n",
    "\n",
    "torch.manual_seed(20200823)\n",
    "np.random.seed(20200823)\n",
    "\n",
    "factor = 1\n",
    "dset = datasets[args.dataset_type](\n",
    "               args.data_dir,\n",
    "               split=\"train\",\n",
    "               device=device,\n",
    "               factor=factor,\n",
    "               n_images=args.n_train,\n",
    "               **config_util.build_data_options(args))\n",
    "\n",
    "if args.background_nlayers > 0 and not dset.should_use_background:\n",
    "    warn('Using a background model for dataset type ' + str(type(dset)) + ' which typically does not use background')\n",
    "\n",
    "dset_test = datasets[args.dataset_type](\n",
    "        args.data_dir, split=\"test\", **config_util.build_data_options(args))\n",
    "\n",
    "global_start_time = datetime.now()\n",
    "\n",
    "# Apply grid scaling to focus 32x32x32 resolution on object region\n",
    "# Scale down scene radius to make grid tighter around object\n",
    "# Handle scene_radius as tensor/array for proper scaling\n",
    "if isinstance(dset.scene_radius, (list, tuple)):\n",
    "    scaled_scene_radius = [r * args.grid_scale_factor for r in dset.scene_radius]\n",
    "else:\n",
    "    # If it's already a tensor\n",
    "    scaled_scene_radius = dset.scene_radius * args.grid_scale_factor\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Amani code \n",
    "\n",
    "# Use FOV to calculate proper intrinsics for 128x128 images\n",
    "fov_degrees = 51.98948897809546  # From your config\n",
    "fov_radians = fov_degrees * np.pi / 180.0\n",
    "image_size = 128  # Your actual image size\n",
    "\n",
    "# Calculate focal length from FOV: focal = (width/2) / tan(fov/2)\n",
    "focal_length = (image_size / 2.0) / np.tan(fov_radians / 2.0)\n",
    "principal_point = image_size / 2.0  # Center of image\n",
    "\n",
    "print(f\"  FOV: {fov_degrees} degrees = {fov_radians} radians\")\n",
    "print(f\"  Image size: {image_size}x{image_size}\")\n",
    "print(f\"  Calculated focal length: {focal_length}\")\n",
    "print(f\"  Principal point: ({principal_point}, {principal_point})\")\n",
    "\n",
    "# Override the intrinsics with FOV-calculated values\n",
    "from util.util import Intrin\n",
    "dset.intrins_full = Intrin(focal_length, focal_length, principal_point, principal_point)\n",
    "dset.intrins = dset.intrins_full\n",
    "\n",
    "dset_test.intrins_full = dset.intrins_full\n",
    "dset_test.intrins = dset_test.intrins_full\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "\n",
    "resample_cameras = [\n",
    "        svox2.Camera(c2w.to(device=device),\n",
    "                     dset.intrins.get('fx', i),\n",
    "                     dset.intrins.get('fy', i),\n",
    "                     dset.intrins.get('cx', i),\n",
    "                     dset.intrins.get('cy', i),\n",
    "                     width=dset.get_image_size(i)[1],\n",
    "                     height=dset.get_image_size(i)[0],\n",
    "                     ndc_coeffs=dset.ndc_coeffs) for i, c2w in enumerate(dset.c2w)\n",
    "    ]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Amani code \n",
    "# Extract camera centers from your cameras\n",
    "camera_centers = torch.stack([cam.c2w[:3, 3] for cam in resample_cameras])\n",
    "\n",
    "# Calculate optimal center and radius\n",
    "optimal_center, optimal_radius = calculate_grid_center_and_radius(\n",
    "    args, camera_centers, resample_cameras)\n",
    "\n",
    "# custom_center = [0.0,0.0, (args.zfar + args.znear)/2]\n",
    "custom_center = [0.0,0.0,0.0]\n",
    "\n",
    "print(f\"Calculated optimal center: {optimal_center}\")\n",
    "print(f\"Calculated optimal radius: {optimal_radius}\")\n",
    "print(f\"Custom center: {custom_center}\")\n",
    "print(f\"Original dataset center: {dset.scene_center}\")\n",
    "\n",
    "print(f\"Original scene radius: {dset.scene_radius}\")\n",
    "print(f\"Scaled scene radius: {scaled_scene_radius} (scale factor: {args.grid_scale_factor})\")\n",
    "\n",
    "# Apply your scale factor to the calculated radius\n",
    "scaled_optimal_radius = [r * args.grid_scale_factor for r in optimal_radius]\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "grid = svox2.SparseGrid(reso=reso_list[reso_id],\n",
    "                        # center=dset.scene_center,\n",
    "                        center=custom_center, # Amani code: use custom center\n",
    "                        radius=scaled_scene_radius,\n",
    "                        use_sphere_bound=dset.use_sphere_bound and not args.nosphereinit,\n",
    "                        # use_sphere_bound=False, # Amani code: use sphere bound\n",
    "                        basis_dim=args.sh_dim,\n",
    "                        use_z_order=True,\n",
    "                        device=device,\n",
    "                        basis_reso=args.basis_reso,\n",
    "                        basis_type=svox2.__dict__['BASIS_TYPE_' + args.basis_type.upper()],\n",
    "                        mlp_posenc_size=args.mlp_posenc_size,\n",
    "                        mlp_width=args.mlp_width,\n",
    "                        background_nlayers=args.background_nlayers,\n",
    "                        background_reso=args.background_reso)\n",
    "\n",
    "# DC -> gray; mind the SH scaling!\n",
    "grid.sh_data.data[:] = 0.0\n",
    "grid.density_data.data[:] = 0.0 if args.lr_fg_begin_step > 0 else args.init_sigma\n",
    "\n",
    "if grid.use_background:\n",
    "    grid.background_data.data[..., -1] = args.init_sigma_bg\n",
    "    #  grid.background_data.data[..., :-1] = 0.5 / svox2.utils.SH_C0\n",
    "\n",
    "#  grid.sh_data.data[:, 0] = 4.0\n",
    "#  osh = grid.density_data.data.shape\n",
    "#  den = grid.density_data.data.view(grid.links.shape)\n",
    "#  #  den[:] = 0.00\n",
    "#  #  den[:, :256, :] = 1e9\n",
    "#  #  den[:, :, 0] = 1e9\n",
    "#  grid.density_data.data = den.view(osh)\n",
    "\n",
    "optim_basis_mlp = None\n",
    "\n",
    "if grid.basis_type == svox2.BASIS_TYPE_3D_TEXTURE:\n",
    "    grid.reinit_learned_bases(init_type='sh')\n",
    "    #  grid.reinit_learned_bases(init_type='fourier')\n",
    "    #  grid.reinit_learned_bases(init_type='sg', upper_hemi=True)\n",
    "    #  grid.basis_data.data.normal_(mean=0.28209479177387814, std=0.001)\n",
    "\n",
    "elif grid.basis_type == svox2.BASIS_TYPE_MLP:\n",
    "    # MLP!\n",
    "    optim_basis_mlp = torch.optim.Adam(\n",
    "                    grid.basis_mlp.parameters(),\n",
    "                    lr=args.lr_basis\n",
    "                )\n",
    "\n",
    "\n",
    "grid.requires_grad_(True)\n",
    "config_util.setup_render_opts(grid.opt, args)\n",
    "print('Render options', grid.opt)\n",
    "\n",
    "gstep_id_base = 0\n",
    "\n",
    "ckpt_path = path.join(args.train_dir, 'ckpt.npz')\n",
    "\n",
    "lr_sigma_func = get_expon_lr_func(args.lr_sigma, args.lr_sigma_final, args.lr_sigma_delay_steps,\n",
    "                                  args.lr_sigma_delay_mult, args.lr_sigma_decay_steps)\n",
    "lr_sh_func = get_expon_lr_func(args.lr_sh, args.lr_sh_final, args.lr_sh_delay_steps,\n",
    "                               args.lr_sh_delay_mult, args.lr_sh_decay_steps)\n",
    "lr_basis_func = get_expon_lr_func(args.lr_basis, args.lr_basis_final, args.lr_basis_delay_steps,\n",
    "                               args.lr_basis_delay_mult, args.lr_basis_decay_steps)\n",
    "lr_sigma_bg_func = get_expon_lr_func(args.lr_sigma_bg, args.lr_sigma_bg_final, args.lr_sigma_bg_delay_steps,\n",
    "                               args.lr_sigma_bg_delay_mult, args.lr_sigma_bg_decay_steps)\n",
    "lr_color_bg_func = get_expon_lr_func(args.lr_color_bg, args.lr_color_bg_final, args.lr_color_bg_delay_steps,\n",
    "                               args.lr_color_bg_delay_mult, args.lr_color_bg_decay_steps)\n",
    "lr_sigma_factor = 1.0\n",
    "lr_sh_factor = 1.0\n",
    "lr_basis_factor = 1.0\n",
    "\n",
    "last_upsamp_step = args.init_iters\n",
    "\n",
    "if args.enable_random:\n",
    "    warn(\"Randomness is enabled for training (normal for LLFF & scenes with background)\")\n",
    "\n",
    "epoch_id = -1\n",
    "\n",
    "# first_vid_path = Path(args.train_dir) / \"video_00000.mp4\"\n",
    "# render_video(grid,\n",
    "#              resample_cameras[:60],   # 60 poses ≈ 5 s @ 12 fps\n",
    "#              first_vid_path,\n",
    "#              fps=12, crop=1) \n",
    "\n",
    "# Add gray density to visualize focused grid bounds in the first video\n",
    "print(\"Setting temporary gray density to visualize grid bounds...\")\n",
    "grid.density_data.data[:] = 100.0  # High density for complete opacity\n",
    "grid.sh_data.data[:] = -0.5        # Negative to counteract +0.5 offset in rendering for black color\n",
    "\n",
    "\n",
    "# Re-render first video with visible grid bounds\n",
    "first_vid_path_bounds = Path(args.train_dir) / \"video_00000_bounds.mp4\" \n",
    "render_video(grid, resample_cameras, first_vid_path_bounds, fps=12, crop=1)\n",
    "\n",
    "# Reset density to proper initial values for training\n",
    "grid.density_data.data[:] = 0.0 if args.lr_fg_begin_step > 0 else args.init_sigma\n",
    "grid.sh_data.data[:] = 0.0  # Reset SH coefficients\n",
    "\n",
    "print(f\"✅ Grid bounds video saved! Density reset to {args.init_sigma if args.lr_fg_begin_step == 0 else 0.0} for training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb7ce64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset.scene_center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "01f1fc18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.sh_data.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "502a6965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.35, 0.35, 0.35]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_scene_radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b63e2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ray-grid intersection lengths: min=227.56741333007812, max=301.2597961425781, mean=270.5103759765625\n",
      "Rays hitting grid: 16384 / 16384\n",
      "Camera position: tensor([ 0.9494, -0.0073, -0.0329], device='cuda:0')\n",
      "Grid center: tensor([0., 0., 0.])\n",
      "Grid radius: tensor([0.8000, 0.8000, 0.8000])\n",
      "Grid world bounds: tensor([-0.8000, -0.8000, -0.8000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/weka/scratch/weka/tenenbaum/akiruga/svox2/svox2/svox2.py:1065: UserWarning: Using slow volume rendering, should only be used for debugging\n",
      "  warn(\"Using slow volume rendering, should only be used for debugging\")\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check ray-grid intersection\n",
    "test_cam = dset.c2w[0].to(device=device)\n",
    "test_camera = svox2.Camera(test_cam, dset.intrins.get('fx', 0), dset.intrins.get('fy', 0), dset.intrins.get('cx', 0), dset.intrins.get('cy', 0), dset.get_image_size(0)[1], dset.get_image_size(0)[0], ndc_coeffs=dset.ndc_coeffs)\n",
    "test_rays = test_camera.gen_rays()\n",
    "\n",
    "# Check where rays intersect the grid bounds\n",
    "ray_lengths = grid.volume_render_image(test_camera, return_raylen=True)\n",
    "print(f\"Ray-grid intersection lengths: min={ray_lengths.min()}, max={ray_lengths.max()}, mean={ray_lengths.mean()}\")\n",
    "print(f\"Rays hitting grid: {(ray_lengths > 0).sum()} / {ray_lengths.numel()}\")\n",
    "\n",
    "# Check actual camera position vs grid\n",
    "print(f\"Camera position: {test_cam[:3, 3]}\")\n",
    "print(f\"Grid center: {grid.center}\")\n",
    "print(f\"Grid radius: {grid.radius}\")\n",
    "print(f\"Grid world bounds: {grid.center - grid.radius}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7be67a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Selecting random rays\n",
      "✅ Modified eval_step to save comparison images!\n",
      "Eval step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 114.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval stats: {'psnr': 13.804213922208424, 'mse': 0.04273673923065265}\n",
      "📸 Saved comparison image: /om/user/akiruga/svox2/data/ckpts/shapenet_chairs_jupyter/comparison_epoch_0000.png\n",
      "Train step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 psnr=30.36: 100%|████████████████████████████████████████████████████████████████████████████████| 12800/12800 [00:12<00:00, 1008.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️  Saved preview video → /om/user/akiruga/svox2/data/ckpts/shapenet_chairs_jupyter/video_000000.mp4\n",
      "Saving /om/user/akiruga/svox2/data/ckpts/shapenet_chairs_jupyter/ckpt.npz\n",
      " Selecting random rays\n",
      "✅ Modified eval_step to save comparison images!\n",
      "Eval step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:00<00:00, 121.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval stats: {'psnr': 30.6643928369251, 'mse': 0.0009517879932120975}\n",
      "📸 Saved comparison image: /om/user/akiruga/svox2/data/ckpts/shapenet_chairs_jupyter/comparison_epoch_0001.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 psnr=31.58: 100%|████████████████████████████████████████████████████████████████████████████████| 12800/12800 [00:12<00:00, 1015.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️  Saved preview video → /om/user/akiruga/svox2/data/ckpts/shapenet_chairs_jupyter/video_012800.mp4\n",
      " Selecting random rays\n",
      "✅ Modified eval_step to save comparison images!\n",
      "Eval step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:00<00:00, 137.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval stats: {'psnr': 31.08596237789126, 'mse': 0.0008692962264398201}\n",
      "📸 Saved comparison image: /om/user/akiruga/svox2/data/ckpts/shapenet_chairs_jupyter/comparison_epoch_0002.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2 psnr=30.36: 100%|████████████████████████████████████████████████████████████████████████████████| 12800/12800 [00:11<00:00, 1124.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️  Saved preview video → /om/user/akiruga/svox2/data/ckpts/shapenet_chairs_jupyter/video_025600.mp4\n",
      " Selecting random rays\n",
      "✅ Modified eval_step to save comparison images!\n",
      "Eval step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:01<00:00, 18.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval stats: {'psnr': 31.367108222598535, 'mse': 0.0008256221772171557}\n",
      "📸 Saved comparison image: /om/user/akiruga/svox2/data/ckpts/shapenet_chairs_jupyter/comparison_epoch_0003.png\n",
      "Train step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3 psnr=30.69: 100%|█████████████████████████████████████████████████████████████████████████████████| 12800/12800 [00:14<00:00, 869.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️  Saved preview video → /om/user/akiruga/svox2/data/ckpts/shapenet_chairs_jupyter/video_038400.mp4\n",
      " Selecting random rays\n",
      "✅ Modified eval_step to save comparison images!\n",
      "Eval step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:00<00:00, 121.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval stats: {'psnr': 31.519597494449105, 'mse': 0.0008036131940094666}\n",
      "📸 Saved comparison image: /om/user/akiruga/svox2/data/ckpts/shapenet_chairs_jupyter/comparison_epoch_0004.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 4 psnr=32.36: 100%|█████████████████████████████████████████████████████████████████████████████████| 12800/12800 [00:19<00:00, 649.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️  Saved preview video → /om/user/akiruga/svox2/data/ckpts/shapenet_chairs_jupyter/video_051200.mp4\n",
      " Selecting random rays\n",
      "✅ Modified eval_step to save comparison images!\n",
      "Eval step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:00<00:00, 127.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval stats: {'psnr': 31.66630375900876, 'mse': 0.0007820925578319778}\n",
      "📸 Saved comparison image: /om/user/akiruga/svox2/data/ckpts/shapenet_chairs_jupyter/comparison_epoch_0005.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 5 psnr=30.79: 100%|█████████████████████████████████████████████████████████████████████████████████| 12800/12800 [00:15<00:00, 811.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️  Saved preview video → /om/user/akiruga/svox2/data/ckpts/shapenet_chairs_jupyter/video_064000.mp4\n",
      "Saving /om/user/akiruga/svox2/data/ckpts/shapenet_chairs_jupyter/ckpt.npz\n",
      " Selecting random rays\n",
      "✅ Modified eval_step to save comparison images!\n",
      "Eval step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:00<00:00, 120.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval stats: {'psnr': 31.799419387116988, 'mse': 0.0007593164752636637}\n",
      "📸 Saved comparison image: /om/user/akiruga/svox2/data/ckpts/shapenet_chairs_jupyter/comparison_epoch_0006.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 6 psnr=31.47: 100%|████████████████████████████████████████████████████████████████████████████████| 12800/12800 [00:11<00:00, 1112.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️  Saved preview video → /om/user/akiruga/svox2/data/ckpts/shapenet_chairs_jupyter/video_076800.mp4\n",
      " Selecting random rays\n",
      "✅ Modified eval_step to save comparison images!\n",
      "Eval step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:00<00:00, 122.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval stats: {'psnr': 31.925693490703658, 'mse': 0.0007425584231636354}\n",
      "📸 Saved comparison image: /om/user/akiruga/svox2/data/ckpts/shapenet_chairs_jupyter/comparison_epoch_0007.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 7 psnr=31.27: 100%|█████████████████████████████████████████████████████████████████████████████████| 12800/12800 [00:13<00:00, 951.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️  Saved preview video → /om/user/akiruga/svox2/data/ckpts/shapenet_chairs_jupyter/video_089600.mp4\n",
      " Selecting random rays\n",
      "✅ Modified eval_step to save comparison images!\n",
      "Eval step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:00<00:00, 113.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval stats: {'psnr': 32.00629797208839, 'mse': 0.0007330817101146316}\n",
      "📸 Saved comparison image: /om/user/akiruga/svox2/data/ckpts/shapenet_chairs_jupyter/comparison_epoch_0008.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 8 psnr=32.50: 100%|█████████████████████████████████████████████████████████████████████████████████| 12800/12800 [00:15<00:00, 814.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️  Saved preview video → /om/user/akiruga/svox2/data/ckpts/shapenet_chairs_jupyter/video_102400.mp4\n",
      " Selecting random rays\n",
      "✅ Modified eval_step to save comparison images!\n",
      "Eval step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:00<00:00, 121.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval stats: {'psnr': 32.070105707056165, 'mse': 0.0007261667425544667}\n",
      "📸 Saved comparison image: /om/user/akiruga/svox2/data/ckpts/shapenet_chairs_jupyter/comparison_epoch_0009.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 9 psnr=33.70: 100%|█████████████████████████████████████████████████████████████████████████████████| 12800/12800 [00:14<00:00, 870.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️  Saved preview video → /om/user/akiruga/svox2/data/ckpts/shapenet_chairs_jupyter/video_115200.mp4\n",
      "* Final eval and save\n",
      "Eval step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:00<00:00, 119.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval stats: {'psnr': 32.11459061595856, 'mse': 0.0007181182902838502}\n",
      "📸 Saved comparison image: /om/user/akiruga/svox2/data/ckpts/shapenet_chairs_jupyter/comparison_epoch_0009.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dset_test = dset\n",
    "while True:\n",
    "    dset.shuffle_rays()\n",
    "    epoch_id += 1\n",
    "    epoch_size = dset.rays.origins.size(0)\n",
    "    batches_per_epoch = (epoch_size-1)//args.batch_size+1\n",
    "    # Test\n",
    "    def eval_step():\n",
    "        # Put in a function to avoid memory leak\n",
    "        print('Eval step')\n",
    "        with torch.no_grad():\n",
    "            stats_test = {'psnr' : 0.0, 'mse' : 0.0}\n",
    "\n",
    "            # Standard set\n",
    "            N_IMGS_TO_EVAL = min(20 if epoch_id > 0 else 5, dset_test.n_images)\n",
    "            N_IMGS_TO_SAVE = N_IMGS_TO_EVAL # if not args.tune_mode else 1\n",
    "            img_eval_interval = dset_test.n_images // N_IMGS_TO_EVAL\n",
    "            img_save_interval = (N_IMGS_TO_EVAL // N_IMGS_TO_SAVE)\n",
    "            img_ids = range(0, dset_test.n_images, img_eval_interval)\n",
    "\n",
    "            # Special 'very hard' specular + fuzz set\n",
    "            #  img_ids = [2, 5, 7, 9, 21,\n",
    "            #             44, 45, 47, 49, 56,\n",
    "            #             80, 88, 99, 115, 120,\n",
    "            #             154]\n",
    "            img_save_interval = 1\n",
    "\n",
    "            n_images_gen = 0\n",
    "            for i, img_id in tqdm(enumerate(img_ids), total=len(img_ids)):\n",
    "                c2w = dset_test.c2w[img_id].to(device=device)\n",
    "                cam = svox2.Camera(c2w,\n",
    "                                   dset_test.intrins.get('fx', img_id),\n",
    "                                   dset_test.intrins.get('fy', img_id),\n",
    "                                   dset_test.intrins.get('cx', img_id),\n",
    "                                   dset_test.intrins.get('cy', img_id),\n",
    "                                   width=dset_test.get_image_size(img_id)[1],\n",
    "                                   height=dset_test.get_image_size(img_id)[0],\n",
    "                                   ndc_coeffs=dset_test.ndc_coeffs)\n",
    "                rgb_pred_test = grid.volume_render_image(cam, use_kernel=True)\n",
    "                rgb_gt_test = dset_test.gt[img_id].to(device=device)\n",
    "                all_mses = ((rgb_gt_test - rgb_pred_test) ** 2).cpu()\n",
    "                if i % img_save_interval == 0:\n",
    "                    img_pred = rgb_pred_test.cpu()\n",
    "                    img_pred.clamp_max_(1.0)\n",
    "                    summary_writer.add_image(f'test/image_{img_id:04d}',\n",
    "                            img_pred, global_step=gstep_id_base, dataformats='HWC')\n",
    "                    if args.log_mse_image:\n",
    "                        mse_img = all_mses / all_mses.max()\n",
    "                        summary_writer.add_image(f'test/mse_map_{img_id:04d}',\n",
    "                                mse_img, global_step=gstep_id_base, dataformats='HWC')\n",
    "                    if args.log_depth_map:\n",
    "                        depth_img = grid.volume_render_depth_image(cam,\n",
    "                                    args.log_depth_map_use_thresh if\n",
    "                                    args.log_depth_map_use_thresh else None\n",
    "                                )\n",
    "                        depth_img = viridis_cmap(depth_img.cpu())\n",
    "                        summary_writer.add_image(f'test/depth_map_{img_id:04d}',\n",
    "                                depth_img,\n",
    "                                global_step=gstep_id_base, dataformats='HWC')\n",
    "\n",
    "                rgb_pred_test = rgb_gt_test = None\n",
    "                mse_num : float = all_mses.mean().item()\n",
    "                psnr = -10.0 * math.log10(mse_num)\n",
    "                if math.isnan(psnr):\n",
    "                    print('NAN PSNR', i, img_id, mse_num)\n",
    "                    assert False\n",
    "                stats_test['mse'] += mse_num\n",
    "                stats_test['psnr'] += psnr\n",
    "                n_images_gen += 1\n",
    "\n",
    "            if grid.basis_type == svox2.BASIS_TYPE_3D_TEXTURE or \\\n",
    "               grid.basis_type == svox2.BASIS_TYPE_MLP:\n",
    "                 # Add spherical map visualization\n",
    "                EQ_RESO = 256\n",
    "                eq_dirs = generate_dirs_equirect(EQ_RESO * 2, EQ_RESO)\n",
    "                eq_dirs = torch.from_numpy(eq_dirs).to(device=device).view(-1, 3)\n",
    "\n",
    "                if grid.basis_type == svox2.BASIS_TYPE_MLP:\n",
    "                    sphfuncs = grid._eval_basis_mlp(eq_dirs)\n",
    "                else:\n",
    "                    sphfuncs = grid._eval_learned_bases(eq_dirs)\n",
    "                sphfuncs = sphfuncs.view(EQ_RESO, EQ_RESO*2, -1).permute([2, 0, 1]).cpu().numpy()\n",
    "\n",
    "                stats = [(sphfunc.min(), sphfunc.mean(), sphfunc.max())\n",
    "                        for sphfunc in sphfuncs]\n",
    "                sphfuncs_cmapped = [viridis_cmap(sphfunc) for sphfunc in sphfuncs]\n",
    "                for im, (minv, meanv, maxv) in zip(sphfuncs_cmapped, stats):\n",
    "                    cv2.putText(im, f\"{minv=:.4f} {meanv=:.4f} {maxv=:.4f}\", (10, 20),\n",
    "                                0, 0.5, [255, 0, 0])\n",
    "                sphfuncs_cmapped = np.concatenate(sphfuncs_cmapped, axis=0)\n",
    "                summary_writer.add_image(f'test/spheric',\n",
    "                        sphfuncs_cmapped, global_step=gstep_id_base, dataformats='HWC')\n",
    "                # END add spherical map visualization\n",
    "\n",
    "            stats_test['mse'] /= n_images_gen\n",
    "            stats_test['psnr'] /= n_images_gen\n",
    "            for stat_name in stats_test:\n",
    "                summary_writer.add_scalar('test/' + stat_name,\n",
    "                        stats_test[stat_name], global_step=gstep_id_base)\n",
    "            summary_writer.add_scalar('epoch_id', float(epoch_id), global_step=gstep_id_base)\n",
    "            print('eval stats:', stats_test)\n",
    "    # Modify the eval_step call to include comparison image saving\n",
    "    original_eval_step = eval_step\n",
    "\n",
    "    def eval_step_with_comparison():\n",
    "        # Run the original eval step\n",
    "        original_eval_step()\n",
    "        \n",
    "        # Save comparison image after eval\n",
    "        save_comparison_image(grid, dset_test, epoch_id, args, device)\n",
    "\n",
    "    # Replace the eval_step function\n",
    "    eval_step = eval_step_with_comparison\n",
    "\n",
    "    print(\"✅ Modified eval_step to save comparison images!\")\n",
    "\n",
    "    if epoch_id % max(factor, args.eval_every) == 0: #and (epoch_id > 0 or not args.tune_mode):\n",
    "        # NOTE: we do an eval sanity check, if not in tune_mode\n",
    "        eval_step()\n",
    "        gc.collect()\n",
    "        # break # Amani code: break after one eval\n",
    "\n",
    "    def train_step():\n",
    "        print('Train step')\n",
    "        pbar = tqdm(enumerate(range(0, epoch_size, args.batch_size)), total=batches_per_epoch)\n",
    "        stats = {\"mse\" : 0.0, \"psnr\" : 0.0, \"invsqr_mse\" : 0.0}\n",
    "        for iter_id, batch_begin in pbar:\n",
    "            gstep_id = iter_id + gstep_id_base\n",
    "            if args.lr_fg_begin_step > 0 and gstep_id == args.lr_fg_begin_step:\n",
    "                grid.density_data.data[:] = args.init_sigma\n",
    "            lr_sigma = lr_sigma_func(gstep_id) * lr_sigma_factor\n",
    "            lr_sh = lr_sh_func(gstep_id) * lr_sh_factor\n",
    "            lr_basis = lr_basis_func(gstep_id - args.lr_basis_begin_step) * lr_basis_factor\n",
    "            lr_sigma_bg = lr_sigma_bg_func(gstep_id - args.lr_basis_begin_step) * lr_basis_factor\n",
    "            lr_color_bg = lr_color_bg_func(gstep_id - args.lr_basis_begin_step) * lr_basis_factor\n",
    "            if not args.lr_decay:\n",
    "                lr_sigma = args.lr_sigma * lr_sigma_factor\n",
    "                lr_sh = args.lr_sh * lr_sh_factor\n",
    "                lr_basis = args.lr_basis * lr_basis_factor\n",
    "\n",
    "            batch_end = min(batch_begin + args.batch_size, epoch_size)\n",
    "            batch_origins = dset.rays.origins[batch_begin: batch_end]\n",
    "            batch_dirs = dset.rays.dirs[batch_begin: batch_end]\n",
    "            rgb_gt = dset.rays.gt[batch_begin: batch_end]\n",
    "            rays = svox2.Rays(batch_origins, batch_dirs)\n",
    "\n",
    "            #  with Timing(\"volrend_fused\"):\n",
    "            rgb_pred = grid.volume_render_fused(rays, rgb_gt,\n",
    "                    beta_loss=args.lambda_beta,\n",
    "                    sparsity_loss=args.lambda_sparsity,\n",
    "                    randomize=args.enable_random)\n",
    "\n",
    "            #  with Timing(\"loss_comp\"):\n",
    "            mse = F.mse_loss(rgb_gt, rgb_pred)\n",
    "\n",
    "            # Stats\n",
    "            mse_num : float = mse.detach().item()\n",
    "            psnr = -10.0 * math.log10(mse_num)\n",
    "            stats['mse'] += mse_num\n",
    "            stats['psnr'] += psnr\n",
    "            stats['invsqr_mse'] += 1.0 / mse_num ** 2\n",
    "\n",
    "            if (iter_id + 1) % args.print_every == 0:\n",
    "                # Print averaged stats\n",
    "                pbar.set_description(f'epoch {epoch_id} psnr={psnr:.2f}')\n",
    "                for stat_name in stats:\n",
    "                    stat_val = stats[stat_name] / args.print_every\n",
    "                    summary_writer.add_scalar(stat_name, stat_val, global_step=gstep_id)\n",
    "                    stats[stat_name] = 0.0\n",
    "                #  if args.lambda_tv > 0.0:\n",
    "                #      with torch.no_grad():\n",
    "                #          tv = grid.tv(logalpha=args.tv_logalpha, ndc_coeffs=dset.ndc_coeffs)\n",
    "                #      summary_writer.add_scalar(\"loss_tv\", tv, global_step=gstep_id)\n",
    "                #  if args.lambda_tv_sh > 0.0:\n",
    "                #      with torch.no_grad():\n",
    "                #          tv_sh = grid.tv_color()\n",
    "                #      summary_writer.add_scalar(\"loss_tv_sh\", tv_sh, global_step=gstep_id)\n",
    "                #  with torch.no_grad():\n",
    "                #      tv_basis = grid.tv_basis() #  summary_writer.add_scalar(\"loss_tv_basis\", tv_basis, global_step=gstep_id)\n",
    "                summary_writer.add_scalar(\"lr_sh\", lr_sh, global_step=gstep_id)\n",
    "                summary_writer.add_scalar(\"lr_sigma\", lr_sigma, global_step=gstep_id)\n",
    "                if grid.basis_type == svox2.BASIS_TYPE_3D_TEXTURE:\n",
    "                    summary_writer.add_scalar(\"lr_basis\", lr_basis, global_step=gstep_id)\n",
    "                if grid.use_background:\n",
    "                    summary_writer.add_scalar(\"lr_sigma_bg\", lr_sigma_bg, global_step=gstep_id)\n",
    "                    summary_writer.add_scalar(\"lr_color_bg\", lr_color_bg, global_step=gstep_id)\n",
    "\n",
    "                if args.weight_decay_sh < 1.0:\n",
    "                    grid.sh_data.data *= args.weight_decay_sigma\n",
    "                if args.weight_decay_sigma < 1.0:\n",
    "                    grid.density_data.data *= args.weight_decay_sh\n",
    "\n",
    "            #  # For outputting the % sparsity of the gradient\n",
    "            #  indexer = grid.sparse_sh_grad_indexer\n",
    "            #  if indexer is not None:\n",
    "            #      if indexer.dtype == torch.bool:\n",
    "            #          nz = torch.count_nonzero(indexer)\n",
    "            #      else:\n",
    "            #          nz = indexer.size()\n",
    "            #      with open(os.path.join(args.train_dir, 'grad_sparsity.txt'), 'a') as sparsity_file:\n",
    "            #          sparsity_file.write(f\"{gstep_id} {nz}\\n\")\n",
    "\n",
    "            # Apply TV/Sparsity regularizers\n",
    "            if args.lambda_tv > 0.0:\n",
    "                #  with Timing(\"tv_inpl\"):\n",
    "                grid.inplace_tv_grad(grid.density_data.grad,\n",
    "                        scaling=args.lambda_tv,\n",
    "                        sparse_frac=args.tv_sparsity,\n",
    "                        logalpha=args.tv_logalpha,\n",
    "                        ndc_coeffs=dset.ndc_coeffs,\n",
    "                        contiguous=args.tv_contiguous)\n",
    "            if args.lambda_tv_sh > 0.0:\n",
    "                #  with Timing(\"tv_color_inpl\"):\n",
    "                grid.inplace_tv_color_grad(grid.sh_data.grad,\n",
    "                        scaling=args.lambda_tv_sh,\n",
    "                        sparse_frac=args.tv_sh_sparsity,\n",
    "                        ndc_coeffs=dset.ndc_coeffs,\n",
    "                        contiguous=args.tv_contiguous)\n",
    "            if args.lambda_tv_lumisphere > 0.0:\n",
    "                grid.inplace_tv_lumisphere_grad(grid.sh_data.grad,\n",
    "                        scaling=args.lambda_tv_lumisphere,\n",
    "                        dir_factor=args.tv_lumisphere_dir_factor,\n",
    "                        sparse_frac=args.tv_lumisphere_sparsity,\n",
    "                        ndc_coeffs=dset.ndc_coeffs)\n",
    "            if args.lambda_l2_sh > 0.0:\n",
    "                grid.inplace_l2_color_grad(grid.sh_data.grad,\n",
    "                        scaling=args.lambda_l2_sh)\n",
    "            if grid.use_background and (args.lambda_tv_background_sigma > 0.0 or args.lambda_tv_background_color > 0.0):\n",
    "                grid.inplace_tv_background_grad(grid.background_data.grad,\n",
    "                        scaling=args.lambda_tv_background_color,\n",
    "                        scaling_density=args.lambda_tv_background_sigma,\n",
    "                        sparse_frac=args.tv_background_sparsity,\n",
    "                        contiguous=args.tv_contiguous)\n",
    "            if args.lambda_tv_basis > 0.0:\n",
    "                tv_basis = grid.tv_basis()\n",
    "                loss_tv_basis = tv_basis * args.lambda_tv_basis\n",
    "                loss_tv_basis.backward()\n",
    "            #  print('nz density', torch.count_nonzero(grid.sparse_grad_indexer).item(),\n",
    "            #        ' sh', torch.count_nonzero(grid.sparse_sh_grad_indexer).item())\n",
    "\n",
    "            # Manual SGD/rmsprop step\n",
    "            if gstep_id >= args.lr_fg_begin_step:\n",
    "                grid.optim_density_step(lr_sigma, beta=args.rms_beta, optim=args.sigma_optim)\n",
    "                grid.optim_sh_step(lr_sh, beta=args.rms_beta, optim=args.sh_optim)\n",
    "            if grid.use_background:\n",
    "                grid.optim_background_step(lr_sigma_bg, lr_color_bg, beta=args.rms_beta, optim=args.bg_optim)\n",
    "            if gstep_id >= args.lr_basis_begin_step:\n",
    "                if grid.basis_type == svox2.BASIS_TYPE_3D_TEXTURE:\n",
    "                    grid.optim_basis_step(lr_basis, beta=args.rms_beta, optim=args.basis_optim)\n",
    "                elif grid.basis_type == svox2.BASIS_TYPE_MLP:\n",
    "                    optim_basis_mlp.step()\n",
    "                    optim_basis_mlp.zero_grad()\n",
    "\n",
    "    train_step()\n",
    "    gc.collect()\n",
    "    \n",
    "    # if epoch_id == 0:        # after one full epoch you have some colour\n",
    "    step_vid_path = Path(args.train_dir) / f\"video_{gstep_id_base:06d}.mp4\"\n",
    "    render_video(grid, resample_cameras[:60], step_vid_path, fps=12, crop=1.0)\n",
    "    \n",
    "    gstep_id_base += batches_per_epoch\n",
    "\n",
    "    #  ckpt_path = path.join(args.train_dir, f'ckpt_{epoch_id:05d}.npz')\n",
    "    # Overwrite prev checkpoints since they are very huge\n",
    "    if args.save_every > 0 and (epoch_id) % max(\n",
    "            factor, args.save_every) == 0 and not args.tune_mode:\n",
    "        print('Saving', ckpt_path)\n",
    "        grid.save(ckpt_path)\n",
    "\n",
    "    if (gstep_id_base - last_upsamp_step) >= args.upsamp_every:\n",
    "        last_upsamp_step = gstep_id_base\n",
    "        if reso_id < len(reso_list) - 1:\n",
    "            print('* Upsampling from', reso_list[reso_id], 'to', reso_list[reso_id + 1])\n",
    "            if args.tv_early_only > 0:\n",
    "                print('turning off TV regularization')\n",
    "                args.lambda_tv = 0.0\n",
    "                args.lambda_tv_sh = 0.0\n",
    "            elif args.tv_decay != 1.0:\n",
    "                args.lambda_tv *= args.tv_decay\n",
    "                args.lambda_tv_sh *= args.tv_decay\n",
    "\n",
    "            reso_id += 1\n",
    "            use_sparsify = True\n",
    "            z_reso = reso_list[reso_id] if isinstance(reso_list[reso_id], int) else reso_list[reso_id][2]\n",
    "            grid.resample(reso=reso_list[reso_id],\n",
    "                    sigma_thresh=args.density_thresh,\n",
    "                    weight_thresh=args.weight_thresh / z_reso if use_sparsify else 0.0,\n",
    "                    dilate=2, #use_sparsify,\n",
    "                    cameras=resample_cameras if args.thresh_type == 'weight' else None,\n",
    "                    max_elements=args.max_grid_elements)\n",
    "\n",
    "            if grid.use_background and reso_id <= 1:\n",
    "                grid.sparsify_background(args.background_density_thresh)\n",
    "\n",
    "            if args.upsample_density_add:\n",
    "                grid.density_data.data[:] += args.upsample_density_add\n",
    "\n",
    "        if factor > 1 and reso_id < len(reso_list) - 1:\n",
    "            print('* Using higher resolution images due to large grid; new factor', factor)\n",
    "            factor //= 2\n",
    "            dset.gen_rays(factor=factor)\n",
    "            dset.shuffle_rays()\n",
    "\n",
    "    if gstep_id_base >= args.n_iters:\n",
    "        print('* Final eval and save')\n",
    "        eval_step()\n",
    "        global_stop_time = datetime.now()\n",
    "        secs = (global_stop_time - global_start_time).total_seconds()\n",
    "        timings_file = open(os.path.join(args.train_dir, 'time_mins.txt'), 'a')\n",
    "        timings_file.write(f\"{secs / 60}\\n\")\n",
    "        if not args.tune_nosave:\n",
    "            grid.save(ckpt_path)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e263b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting temporary gray density to visualize grid bounds...\n",
      "✔️  Saved preview video → /om/user/akiruga/svox2/data/ckpts/shapenet_chairs_jupyter/video_102400.mp4\n"
     ]
    }
   ],
   "source": [
    "print(\"Setting temporary gray density to visualize grid bounds...\")\n",
    "grid.density_data.data[:] = 100.0  # High density for complete opacity\n",
    "grid.sh_data.data[:] = -0.5        # Negative to counteract +0.5 offset in rendering for black color\n",
    "\n",
    "\n",
    "\n",
    "step_vid_path = Path(args.train_dir) / f\"video_{gstep_id_base:06d}.mp4\"\n",
    "render_video(grid, resample_cameras, step_vid_path, fps=12, crop=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c2c96be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2169032, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.density_data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1df71096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass 1/2 (density)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1211.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass 2/2 (color), eval 27081 sparse pts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1608.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " New cap: 27081\n",
      "density torch.Size([27081]) torch.float32\n",
      "sh torch.Size([27081, 27]) torch.float32\n",
      "links torch.Size([32768]) torch.int32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# resample to 32x32x32\n",
    "grid_32 = grid.resample(\n",
    "    reso=[32, 32, 32],   # target voxel resolution\n",
    "    sigma_thresh=0.0,    # keep every voxel that has *any* density\n",
    "    dilate=0,            # no neighbour dilation → strictly 32³\n",
    "    use_z_order=False,   # leave False unless you really need Morton layout\n",
    "    accelerate=True      # rebuild distance‑transform table for fast ray‑march kernels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bcf49792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️  Saved preview video → /om/user/akiruga/svox2/data/ckpts/shapenet_chairs_jupyter/video_051200_32.mp4\n"
     ]
    }
   ],
   "source": [
    "step_vid_path = Path(args.train_dir) / f\"video_{gstep_id_base:06d}_32.mp4\"\n",
    "render_video(grid, resample_cameras, step_vid_path, fps=12, crop=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40ed88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid.sh_data.data = grid.sh_data.data/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02674bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 512, 512, 28])\n"
     ]
    }
   ],
   "source": [
    "def get_dense_density_sh(grid):\n",
    "    \"\"\"\n",
    "    Returns a dense 4D grid with shape (X, Y, Z, C), where:\n",
    "    - C = 1 + basis_dim * 3\n",
    "    - Channel 0: density\n",
    "    - Channels 1: density SH coefficients (flattened)\n",
    "    \"\"\"\n",
    "    X, Y, Z = grid.links.shape\n",
    "    C = 1 + grid.sh_data.shape[1]  # 1 for density + SH channels\n",
    "    dense_grid = torch.zeros((X, Y, Z, C), device=grid.links.device)\n",
    "\n",
    "    mask = grid.links >= 0\n",
    "    active_indices = grid.links[mask]\n",
    "\n",
    "    dense_grid_flat = dense_grid.view(-1, C)\n",
    "    mask_flat = mask.view(-1)\n",
    "\n",
    "    # Fill density\n",
    "    dense_grid_flat[mask_flat, 0] = grid.density_data.detach()[active_indices, 0]\n",
    "\n",
    "    # Fill SH coefficients\n",
    "    dense_grid_flat[mask_flat, 1:] = grid.sh_data.detach()[active_indices]\n",
    "\n",
    "    dense_grid = dense_grid_flat.view(X, Y, Z, C)\n",
    "    return dense_grid\n",
    "\n",
    "dense_grid = get_dense_density_sh(grid)\n",
    "print(dense_grid.shape)  # (512, 512, 512, 1 + basis_dim * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d604696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import svox2          # the library that defines SparseGrid\n",
    "from torch import nn\n",
    "from typing import Tuple\n",
    "\n",
    "def dense_to_sparsegrid(\n",
    "        dense_grid: torch.Tensor,\n",
    "        template_grid: svox2.SparseGrid,\n",
    "        device: torch.device = None\n",
    "    ) -> svox2.SparseGrid:\n",
    "    \"\"\"\n",
    "    Reconstruct a fully–dense SparseGrid from a dense tensor.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    dense_grid : (X, Y, Z, 1 + basis_dim*3) tensor\n",
    "        Channel 0 = density, 1: = SH coefficients (flattened RGB·basis_dim).\n",
    "    template_grid : SparseGrid\n",
    "        Any existing grid whose radius/center/render‑opts you want to clone.\n",
    "    device : torch.device, optional\n",
    "        Target device. Defaults to template_grid’s device.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    new_grid : SparseGrid\n",
    "        A grid ready to be used with all rendering functions.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = template_grid.sh_data.device\n",
    "\n",
    "    # ── sizes ────────────────────────────────────────────────────────────────────\n",
    "    X, Y, Z, C = dense_grid.shape\n",
    "    basis_dim = (C - 1) // 3\n",
    "\n",
    "    # ── flatten & split ─────────────────────────────────────────────────────────\n",
    "    flat = dense_grid.view(-1, C)               # (N, C)  where N = X*Y*Z\n",
    "    density_data = flat[:, :1].contiguous()     # (N, 1)\n",
    "    sh_data      = flat[:, 1:].contiguous()     # (N, basis_dim*3)\n",
    "\n",
    "    # ── make a brand‑new SparseGrid with identical meta‑data ────────────────────\n",
    "    new_grid = svox2.SparseGrid(\n",
    "        reso=(X, Y, Z),\n",
    "        radius=template_grid.radius.tolist(),\n",
    "        center=template_grid.center.tolist(),\n",
    "        basis_type=svox2.BASIS_TYPE_SH,\n",
    "        basis_dim=basis_dim,\n",
    "        use_z_order=False,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # overwrite internal tensors\n",
    "    new_grid.density_data = nn.Parameter(density_data.to(device))\n",
    "    new_grid.sh_data      = nn.Parameter(sh_data.to(device))\n",
    "\n",
    "    # every voxel is active → simple dense mapping\n",
    "    n_vox = X * Y * Z\n",
    "    new_grid.links = torch.arange(\n",
    "        n_vox, dtype=torch.int32, device=device).view(X, Y, Z)\n",
    "    new_grid.capacity = n_vox\n",
    "\n",
    "    # optional: accelerate for CUDA ray‑march kernels\n",
    "    if new_grid.links.is_cuda:\n",
    "        new_grid.accelerate()\n",
    "\n",
    "    # copy render options (step size, background, etc.)\n",
    "    new_grid.opt = template_grid.opt\n",
    "\n",
    "    return new_grid\n",
    "\n",
    "# grid = svox2.SparseGrid(reso=reso_list[reso_id],\n",
    "#                         center=dset.scene_center,\n",
    "#                         radius=dset.scene_radius,\n",
    "#                         use_sphere_bound=dset.use_sphere_bound and not args.nosphereinit,\n",
    "#                         basis_dim=args.sh_dim,\n",
    "#                         use_z_order=True,\n",
    "#                         device=device,\n",
    "#                         basis_reso=args.basis_reso,\n",
    "#                         basis_type=svox2.__dict__['BASIS_TYPE_' + args.basis_type.upper()],\n",
    "#                         mlp_posenc_size=args.mlp_posenc_size,\n",
    "#                         mlp_width=args.mlp_width,\n",
    "#                         background_nlayers=args.background_nlayers,\n",
    "#                         background_reso=args.background_reso)\n",
    "\n",
    "# dense_grid = get_dense_density_sh(grid)  # your earlier helper\n",
    "# new_grid   = dense_to_sparsegrid(dense_grid, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480240c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb533634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️  Saved preview video → /om/user/akiruga/svox2/data/ckpts/shapenet_chairs_jupyter/video_128000_dense.mp4\n"
     ]
    }
   ],
   "source": [
    "step_vid_path = Path(args.train_dir) / f\"video_{gstep_id_base:06d}_dense.mp4\"\n",
    "render_video(grid, resample_cameras, step_vid_path, fps=12, crop=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c75d8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get min and max resolution of sparse grid\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "import math \n",
    "\n",
    "ckpt_dir = Path(\"/weka/scratch/weka/tenenbaum/akiruga/svox2/data/ckpts/shapenet_chairs_all_jupyter\")\n",
    "ckpt_paths = sorted(glob.glob(str(ckpt_dir / \"*\" / \"ckpt.npz\")))\n",
    "min_sz = 1024\n",
    "max_sz = 0\n",
    "for ckpt_path in ckpt_paths:\n",
    "    print(f\"Loading checkpoint: {ckpt_path}\")\n",
    "    ckpt = np.load(ckpt_path)\n",
    "    # list all keys \n",
    "    density_data = ckpt[\"density_data\"]\n",
    "    print(density_data.shape)\n",
    "    # cuberoot\n",
    "    min_sz = min(min_sz, pow(density_data.shape[0], 1/3))\n",
    "    max_sz = max(max_sz, pow(density_data.shape[0], 1/3))\n",
    "    break \n",
    "print(min_sz, max_sz)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8678fa29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint: /weka/scratch/weka/tenenbaum/akiruga/svox2/data/ckpts/shapenet_chairs_all_jupyter/1006be65e7bc937e9141f9b58470d646/dense_grid.npz\n"
     ]
    }
   ],
   "source": [
    "# test rendering of saved dense grid \n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "import math \n",
    "\n",
    "ckpt_dir = Path(\"/weka/scratch/weka/tenenbaum/akiruga/svox2/data/ckpts/shapenet_chairs_all_jupyter\")\n",
    "ckpt_paths = sorted(glob.glob(str(ckpt_dir / \"*\" / \"dense_grid.npz\")))\n",
    "min_sz = 1024\n",
    "max_sz = 0\n",
    "for ckpt_path in ckpt_paths:\n",
    "    print(f\"Loading checkpoint: {ckpt_path}\")\n",
    "    ckpt = np.load(ckpt_path)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58ffd538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 32, 32, 28])\n"
     ]
    }
   ],
   "source": [
    "ckpt.keys()\n",
    "dense_grid = torch.from_numpy(ckpt[\"dense_grid\"]).float().to(\"cuda\")\n",
    "shape = ckpt[\"shape\"]\n",
    "center = ckpt[\"center\"]\n",
    "radius = ckpt[\"radius\"]\n",
    "\n",
    "print(dense_grid.shape)\n",
    "\n",
    "new_grid = dense_to_sparsegrid(dense_grid, grid, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f056f411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️  Saved preview video → /weka/scratch/weka/tenenbaum/akiruga/svox2/data/ckpts/shapenet_chairs_jupyter/video_saved_dense_to_sparse.mp4\n"
     ]
    }
   ],
   "source": [
    "# render a video of the new sparse grid \n",
    "step_vid_path = Path(\"/weka/scratch/weka/tenenbaum/akiruga/svox2/data/ckpts/shapenet_chairs_jupyter\") / f\"video_saved_dense_to_sparse.mp4\"\n",
    "render_video(new_grid, resample_cameras, step_vid_path, fps=12, crop=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b595f1a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
